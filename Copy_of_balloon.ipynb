{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Copy of balloon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wwymouu/Background-removal-using-deep-learning/blob/master/Copy_of_balloon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe40RZzbC44J",
        "colab_type": "text"
      },
      "source": [
        "# Mask R-CNN Demo\n",
        "\n",
        "A quick intro to using the pre-trained model to detect and segment objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMkoJH3DDnV0",
        "colab_type": "code",
        "outputId": "2c76fabc-24bc-4c27-9eea-9fcd02288786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4__SpbJxG1xB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16eb3268-b010-44e6-ce2b-cfe9aeccf7ef"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdA9kqFbF1wf",
        "colab_type": "code",
        "outputId": "b5407bb7-5dea-4913-806b-688af921c33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-labEbEuC44K",
        "colab_type": "code",
        "outputId": "9733ed74-6e71-48f1-9a8b-2deb27adb326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime\n",
        "import skimage.draw\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"/content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn import utils\n",
        "from mrcnn.config import Config\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "# Import COCO config\n",
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_WEIGHTS_PATH):\n",
        "    utils.download_trained_weights(COCO_WEIGHTS_PATH)\n",
        "\n",
        "# # Directory of images to run detection on\n",
        "# IMAGE_DIR = os.path.join(\"/content/drive/My Drive/uzh/RL 4 marketing/ebay_dataset/train/\", \"images\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O95BAsKdC44Q",
        "colab_type": "text"
      },
      "source": [
        "## Configurations\n",
        "\n",
        "We'll be using a model trained on the MS-COCO dataset. The configurations of this model are in the ```CocoConfig``` class in ```coco.py```.\n",
        "\n",
        "For inferencing, modify the configurations a bit to fit the task. To do so, sub-class the ```CocoConfig``` class and override the attributes you need to change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sEsNmLYC44R",
        "colab_type": "code",
        "outputId": "df7a55da-70d3-4790-ba0d-87b4acff5939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "class ObjectConfig(Config):\n",
        "    # Set batch size to 1 since we'll be running inference on\n",
        "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "    NAME = \"object\"\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    IMAGES_PER_GPU = 2\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 7  # Background + object\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # Skip detections with < 90% confidence\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "\n",
        "config = ObjectConfig()\n",
        "config.display()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     2\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.9\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 2\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                20\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           object\n",
            "NUM_CLASSES                    8\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                100\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQhD5RJasbp0",
        "colab_type": "text"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOBrOutVsdR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ObjectDataset(utils.Dataset):\n",
        "\n",
        "    def load_object(self, dataset_dir, subset, class_ids=None,):\n",
        "        \"\"\"Load a subset of the Balloon dataset.\n",
        "        dataset_dir: Root directory of the dataset.\n",
        "        subset: Subset to load: train or val\n",
        "        \"\"\"\n",
        "        # # Add classes. We have 7 classes to add.\n",
        "        # self.add_class(\"object\", 1, \"backpack\")\n",
        "        # self.add_class(\"object\", 2, \"bottle\")\n",
        "        # self.add_class(\"object\", 3, \"cup\")\n",
        "        # self.add_class(\"object\", 4, \"laptop\")\n",
        "        # self.add_class(\"object\", 5, \"mouse\")\n",
        "        # self.add_class(\"object\", 6, \"cell phone\")\n",
        "        # self.add_class(\"object\", 7, \"book\")\n",
        "\n",
        "\n",
        "        # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"val\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "        # IMAGE_DIR = os.path.join(dataset_dir, \"images\")\n",
        "        # image_ids = next(os.walk(IMAGE_DIR))[2]\n",
        "        # print(image_ids)\n",
        "\n",
        "        # # Add images\n",
        "        # for image_id in image_ids:\n",
        "        #     self.add_image(\n",
        "        #         \"object\",\n",
        "        #         image_id=image_id,\n",
        "        #         path=os.path.join(dataset_dir, \"images/{}\".format(image_id)))\n",
        "\n",
        "\n",
        "        annFile = os.path.join(dataset_dir, \"coco_instances.json\")\n",
        "        # print(f'Annotation file: {annFile}')\n",
        "        coco = COCO(annFile)\n",
        "\n",
        "        image_dir = os.path.join(dataset_dir, \"images\")\n",
        "        # print(\"image_dir:\", image_dir)  \n",
        "        # Load all classes or a subset?\n",
        "        if not class_ids:\n",
        "            # All classes\n",
        "            class_ids = sorted(coco.getCatIds())\n",
        "\n",
        "        # All images or a subset?\n",
        "        if class_ids:\n",
        "            image_ids = []\n",
        "            for id in class_ids:\n",
        "                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "            # Remove duplicates\n",
        "            image_ids = list(set(image_ids))\n",
        "        else:\n",
        "            # All images\n",
        "            image_ids = list(coco.imgs.keys())\n",
        "\n",
        "        # Add classes\n",
        "        for i in class_ids:\n",
        "            self.add_class(\"object\", i, coco.loadCats(i)[0][\"name\"])\n",
        "        print(\"class_ids\", class_ids)\n",
        "        # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"object\", image_id=i,\n",
        "                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "               \n",
        "    def load_mask(self, image_id):\n",
        "      #   \"\"\"Generate instance masks for an image.\n",
        "      #  Returns:\n",
        "      #   masks: A bool array of shape [height, width, instance count] with\n",
        "      #       one mask per instance.\n",
        "      #   class_ids: a 1D array of class IDs of the instance masks.\n",
        "      #   \"\"\"\n",
        "        # info = self.image_info[image_id]\n",
        "        # print(\"info: \", info)\n",
        "        # # Get mask directory from image path\n",
        "        # mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n",
        "        # print(\"mask_dir: \",mask_dir)\n",
        "        # # Read mask files from .png image\n",
        "        # mask = []\n",
        "        # # class_ids = []\n",
        "        # for f in next(os.walk(mask_dir))[2]:\n",
        "        #     # print(\"f:\", f)\n",
        "        #     if f.endswith(\".png\"):\n",
        "        #         m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)\n",
        "        #         mask.append(m)\n",
        "        # mask = np.stack(mask, axis=-1)\n",
        "        # # class_ids = np.array([self.class_names.index(s[0]) for s in obj])\n",
        "        # # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # # one class ID, we return an array of ones\n",
        "        # return mask #, class_ids.astype(np.int32)\n",
        "\n",
        "\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        # print('image_info',image_info)\n",
        "        if image_info[\"source\"] != \"object\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # print('annotations', annotations)\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = annotation['category_id']\n",
        "            # print(\"class_id\", class_id)\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
        "            class_ids = np.array(class_ids, dtype=np.int32)\n",
        "            return mask, class_ids\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(self.__class__, self, self).load_mask(image_id)\n",
        "                       \n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"object\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "    \n",
        "    def annToRLE(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        segm = ann['segmentation']\n",
        "        if isinstance(segm, list):\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, height, width)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif isinstance(segm['counts'], list):\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, height, width)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann, height, width)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qekEX0pTzSct",
        "colab_type": "code",
        "outputId": "9fbb04df-2aab-483b-cab7-6d50882ad0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "dataset_dir = os.path.abspath(\"/content/drive/My Drive/uzh/RL 4 marketing/ebay_dataset/\")\n",
        "\n",
        "dataset_train = ObjectDataset()\n",
        "dataset_train.load_object(dataset_dir, \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = ObjectDataset()\n",
        "dataset_val.load_object(dataset_dir, \"val\")\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdO99jx8ESVF",
        "colab_type": "code",
        "outputId": "56179c0b-9f24-4072-c085-fda49f43dee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "print(\"image_ids: \", image_ids)\n",
        "for image_id in image_ids:\n",
        "#     # image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "\n",
        "    # visualize.display_top_masks(mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_ids:  [ 31  28 118 111]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uex8QVX00Se",
        "colab_type": "text"
      },
      "source": [
        "##Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS7Y26o-01i3",
        "colab_type": "code",
        "outputId": "8c18286c-2b2c-4e95-e873-9fa915749713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=DEFAULT_LOGS_DIR)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDLO11L41j2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load weights trained on MS COCO, but skip layers that\n",
        "# are different due to the different number of classes\n",
        "# See README for instructions to download the COCO weights\n",
        "model.load_weights(COCO_WEIGHTS_PATH, by_name=True,\n",
        "                    exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                            \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqQxI1Nwca5",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoBRgXULwe6Q",
        "colab_type": "code",
        "outputId": "65a2437e-05cb-44be-bfb7-e7505a44d81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the head branches\n",
        "# Since we're using a very small dataset, and starting from\n",
        "# COCO trained weights, we don't need to train too long. Also,\n",
        "# no need to train all layers, just the heads should do it.\n",
        "print(\"Training network heads\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=30,\n",
        "            layers='heads')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training network heads\n",
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/logs/object20200423T1504/mask_rcnn_object_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 99/100 [============================>.] - ETA: 1s - loss: 1.7859 - rpn_class_loss: 0.0127 - rpn_bbox_loss: 0.2728 - mrcnn_class_loss: 0.3399 - mrcnn_bbox_loss: 0.6916 - mrcnn_mask_loss: 0.4689"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bd7fd9672740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             layers='heads')\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/uzh/RL 4 marketing/Mask_RCNN-master/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m         )\n\u001b[1;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    243\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    399\u001b[0m             outs = model.test_on_batch(x, y,\n\u001b[1;32m    400\u001b[0m                                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                                        reset_metrics=False)\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m   1552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_image_meta to have shape (20,) but got array with shape (19,)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsJbVHWvC44d",
        "colab_type": "text"
      },
      "source": [
        "## Run Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx6ynaYQ26ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferenceConfig(ObjectConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=DEFAULT_LOGS_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_l-1lxji6oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['backpack', 'bottle', 'cup', 'laptop', 'mouse', 'cell phone', 'book']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fXT-WzT5O8r",
        "colab_type": "text"
      },
      "source": [
        "Test on a random image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VKJvnLfqC44d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a4ddf344-0ac0-4a81-8eac-2658b598a58a"
      },
      "source": [
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = os.path.abspath(\"/content/drive/My Drive/uzh/RL 4 marketing/ebay_dataset/output/images\")\n",
        "\n",
        "# Load a random image from the images folder\n",
        "file_names = next(os.walk(IMAGE_DIR))[2]\n",
        "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
        "\n",
        "# Run detection\n",
        "results = model.detect([image], verbose=1)\n",
        "\n",
        "# Visualize results\n",
        "r = results[0]\n",
        "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            class_names, r['scores'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f954615b8dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Run detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Visualize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUIg4_J9w8-3",
        "colab_type": "text"
      },
      "source": [
        "Apply color splash effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qABmVaHDxDl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_splash(image, mask):\n",
        "    \"\"\"Apply color splash effect.\n",
        "    image: RGB image [height, width, 3]\n",
        "    mask: instance segmentation mask [height, width, instance count]\n",
        "    Returns result image.\n",
        "    \"\"\"\n",
        "    # Make a grayscale copy of the image. The grayscale copy still\n",
        "    # has 3 RGB channels, though.\n",
        "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n",
        "    # Copy color pixels from the original color image where mask is set\n",
        "    if mask.shape[-1] > 0:\n",
        "        # We're treating all instances as one, so collapse the mask into one layer\n",
        "        mask = (np.sum(mask, -1, keepdims=True) >= 1)\n",
        "        splash = np.where(mask, image, gray).astype(np.uint8)\n",
        "    else:\n",
        "        splash = gray.astype(np.uint8)\n",
        "    return splash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec7gpaA8xFY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_and_color_splash(model, image):\n",
        "\n",
        "    # Run model detection and generate the color splash effect\n",
        "    print(\"Running on {}\".format(image))\n",
        "    # Read image\n",
        "    image = skimage.io.imread(image)\n",
        "    # Detect objects\n",
        "    r = model.detect([image], verbose=1)[0]\n",
        "    # Color splash\n",
        "    splash = color_splash(image, r['masks'])\n",
        "    # # Save output\n",
        "    # file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n",
        "    # skimage.io.imsave(file_name, splash)\n",
        "    \n",
        "    # print(\"Saved to \", file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhUWmKiv7L9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGb77-A96pNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.detect([image], verbose=0)\n",
        "# Visualize results\n",
        "r = results[0]\n",
        "frame = detect_and_color_splash(model, image)\n",
        "\n",
        "cv2_imshow(frame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0z3mSW8C44h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # This function is used to change the colorful background information to grayscale.\n",
        "# # image[:,:,0] is the Blue channel,image[:,:,1] is the Green channel, image[:,:,2] is the Red channel\n",
        "# # mask == 0 means that this pixel is not belong to the object.\n",
        "# # np.where function means that if the pixel belong to background, change it to gray_image.\n",
        "# # Since the gray_image is 2D, for each pixel in background, we should set 3 channels to the same value to keep the grayscale.\n",
        "# def apply_mask(image, mask):\n",
        "#     image[:, :, 0] = np.where(\n",
        "#         mask == 0,\n",
        "#         125,\n",
        "#         image[:, :, 0]\n",
        "#     )\n",
        "#     image[:, :, 1] = np.where(\n",
        "#         mask == 0,\n",
        "#         12,\n",
        "#         image[:, :, 1]\n",
        "#     )\n",
        "#     image[:, :, 2] = np.where(\n",
        "#         mask == 0,\n",
        "#         15,\n",
        "#         image[:, :, 2]\n",
        "#     )\n",
        "#     return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evI2icaM5qFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # This function is used to show the object detection result in original image.\n",
        "# def display_instances(image, boxes, masks, ids, names, scores):\n",
        "#     # max_area will save the largest object for all the detection results\n",
        "#     max_area = 0\n",
        "    \n",
        "#     # n_instances saves the amount of all objects\n",
        "#     n_instances = boxes.shape[0]\n",
        "\n",
        "#     if not n_instances:\n",
        "#         print('NO INSTANCES TO DISPLAY')\n",
        "#     else:\n",
        "#         assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n",
        "\n",
        "#     for i in range(n_instances):\n",
        "#         if not np.any(boxes[i]):\n",
        "#             continue\n",
        "\n",
        "#         # compute the square of each object\n",
        "#         y1, x1, y2, x2 = boxes[i]\n",
        "#         square = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "#         # use label to select person object from all the 80 classes in COCO dataset\n",
        "#         label = names[ids[i]]\n",
        "#         if label == 'person':\n",
        "#             # save the largest object in the image as main character\n",
        "#             # other people will be regarded as background\n",
        "#             if square > max_area:\n",
        "#                 max_area = square\n",
        "#                 mask = masks[:, :, i]\n",
        "#             else:\n",
        "#                 continue\n",
        "#         else:\n",
        "#             if square > max_area:\n",
        "#                 max_area = square\n",
        "#                 mask = masks[:, :, i]\n",
        "#             else:\n",
        "#                 continue\n",
        "\n",
        "#         # apply mask for the image\n",
        "#     # by mistake you put apply_mask inside for loop or you can write continue in if also\n",
        "#     image = apply_mask(image, mask)\n",
        "        \n",
        "#     return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxhEragS5qQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def transparent_back(image):\n",
        "#     image = image.convert('RGBA')\n",
        "#     L,H = image.size\n",
        "#     color_0 = image.getpixel((0, 0))\n",
        "#     for h in range(H):\n",
        "#         for l in range(L):\n",
        "#             dot = (l, h)\n",
        "#             color_1 = image.getpixel(dot)\n",
        "#             if color_1 == color_0:\n",
        "#                 color_1 = color_1[:-1] + (0,)\n",
        "#                 image.putpixel(dot, color_1)\n",
        "\n",
        "#     return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni9_196o_YG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2C5GrxH6qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if __name__ == \"__main__\":\n",
        "    \n",
        "#     # image = cv2.imread(sys.argv[1], -1)\n",
        "#     # height, width, channels = image.shape\n",
        "#     results = model.detect([image], verbose=0)\n",
        "#     # Visualize results\n",
        "#     r = results[0]\n",
        "#     frame = display_instances(\n",
        "#          image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']\n",
        "#     )\n",
        "\n",
        "#     cv2_imshow(frame)\n",
        "\n",
        "#     # # image = PIL.Image.open(\"./temp.png\")\n",
        "#     # image = transparent_back(frame)\n",
        "#     # # image.save(\"removed_back.png\")\n",
        "#     # plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAt7Z3X66qxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}